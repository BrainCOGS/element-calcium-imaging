{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process calcium imaging data with DataJoint Elements\n",
    "\n",
    "This notebook will walk through processing two-photon calcium imaging data collected\n",
    "from ScanImage and processed with Suite2p.\n",
    "\n",
    "The DataJoint Python API and Element Calcium Imaging offer a lot of features to support collaboration, automation, reproducibility, and visualizations.\n",
    "\n",
    "For more information on these topics, please visit our documentation: \n",
    " \n",
    "- [DataJoint Core](https://datajoint.com/docs/core/): General principles\n",
    "\n",
    "- DataJoint [Python](https://datajoint.com/docs/core/datajoint-python/) and\n",
    "  [MATLAB](https://datajoint.com/docs/core/datajoint-matlab/) APIs: in-depth reviews of\n",
    "  specifics\n",
    "\n",
    "- [DataJoint Element Calcium Imaging](https://datajoint.com/docs/elements/element-calcium-imaging/):\n",
    "  A modular pipeline for calcium imaging analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the packages necessary to run this workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "import datajoint as dj\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Basics:\n",
    "\n",
    "Any DataJoint workflow can be broken down into basic 3 parts:\n",
    "\n",
    "- `Insert`\n",
    "- `Populate` (or process)\n",
    "- `Analyze`\n",
    "\n",
    "In this demo we will:\n",
    "- `Insert` metadata about an animal subject, recording session, and \n",
    "  parameters related to processing calcium imaging data through Suite2p.\n",
    "- `Populate` tables with outputs of image processing including motion correction,\n",
    "  segmentation, mask classification, fluorescence traces and deconvolved activity traces.\n",
    "- `Analyze` the processed data by ***querying*** and plotting activity traces.\n",
    "\n",
    "Each of these topics will be explained thoroughly in this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow diagram\n",
    "\n",
    "This workflow is assembled from 4 DataJoint elements:\n",
    "+ [element-lab](https://github.com/datajoint/element-lab)\n",
    "+ [element-animal](https://github.com/datajoint/element-animal)\n",
    "+ [element-session](https://github.com/datajoint/element-session)\n",
    "+ [element-calcium-imaging](https://github.com/datajoint/element-calcium-imaging)\n",
    "\n",
    "Each element declares its own schema in the database. These schemas can be imported like\n",
    "any other Python package. This workflow is composed of schemas from each of the Elements\n",
    "above and correspond to a module within `workflow_calcium_imaging.pipeline`.\n",
    "\n",
    "The schema diagram is a good reference for understanding the order of the tables\n",
    "within the workflow, as well as the corresponding table type.\n",
    "Let's activate the elements and view the schema diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow_calcium_imaging.pipeline import lab, subject, session, scan, imaging, Equipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    dj.Diagram(subject.Subject)\n",
    "    + dj.Diagram(session.Session)\n",
    "    + dj.Diagram(scan)\n",
    "    + dj.Diagram(imaging)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the diagram above seems complex at first, it becomes more clear when it's\n",
    "approached as a hierarchy of tables that define the order in which the workflow expects to\n",
    "receive data in each of its tables. \n",
    "\n",
    "+ Tables with a green, or rectangular shape expect to receive data manually using the\n",
    "`insert()` function. The tables higher up in the diagram such as `subject.Subject()`\n",
    "should be the first to receive data. This ensures data integrity by preventing orphaned\n",
    "data within DataJoint schemas. \n",
    "+ Tables with a purple oval or red circle can be automatically filled with relevant data\n",
    "  by calling `populate()`. For example `scan.ScanInfo` and its part-table\n",
    "  `scan.ScanInfo.Field` are both populated with `scan.ScanInfo.populate()`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the workflow: Insert\n",
    "\n",
    "### Insert entries into manual tables and populate automated tables\n",
    "\n",
    "To view details about a table's dependencies and attributes, use functions `.describe()`\n",
    "and `.heading`, respectively.\n",
    "\n",
    "Let's start with the first table in the schema diagram (the `subject` table) and view\n",
    "the table attributes we need to insert. There are two ways you can do this: *run each\n",
    "of the two cells below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells above show all attributes of the subject table. These are particularly useful functions if you are new to\n",
    "DataJoint Elements and are unsure of the attributes required for each table. We will insert data into the\n",
    "`subject.Subject` table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.insert1(\n",
    "    dict(\n",
    "        subject=\"subject1\",\n",
    "        sex=\"F\",\n",
    "        subject_birth_date=\"2020-01-01\",\n",
    "        subject_description=\"ScanImage acquisition. Suite2p processing.\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the steps above for the `Session` table and see how the output varies between\n",
    "`.describe` and `.heading`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells above show the dependencies and attributes for the `session.Session` table.\n",
    "Notice that `describe` shows the dependencies of the table on upstream tables. The\n",
    "`Session` table depends on the upstream `Subject` table. Whereas `heading` lists all the\n",
    "attributes of the `Session` table, regardless of whether they are declared in an upstream\n",
    "table. \n",
    "\n",
    "\n",
    "Here we will demonstrate a very useful way of inserting data by assigning the dictionary\n",
    "to a variable `session_key`. This variable can be used to insert entries into tables that\n",
    "contain the `Session` table as one of its attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_key = dict(subject=\"subject1\", session_datetime=\"2021-04-30 12:22:15.032\")\n",
    "\n",
    "session.Session.insert1(session_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.SessionDirectory.insert1(\n",
    "    dict(**session_key,\n",
    "        session_dir=\"subject1/session1\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use `describe` and `heading` for the Scan table. Do you notice anything we\n",
    "might have missed here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.Scan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.Scan.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Scan` table's attributes include the `Session` table **and** the `Equipment` table.\n",
    "Let's see what happens when you try to insert data into `Scan` without inserting into\n",
    "`Equipment` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.Scan.insert1(\n",
    "    dict(\n",
    "        session_key,\n",
    "        scan_id=0,\n",
    "        scanner=\"ScanImage\",\n",
    "        acq_software=\"ScanImage\",\n",
    "        scan_notes=\"\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# This cell will produce an error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataJoint issues an `IntegrityError` because there is no entry in the `Equipment` table. Let's go\n",
    "ahead and insert into the Equipment table after checking its attributes. Then, we will\n",
    "try the insert into `Scan` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Equipment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Equipment.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Equipment.insert1(dict(scanner=\"ScanImage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.Scan.insert1(\n",
    "    dict(\n",
    "        **session_key,\n",
    "        scan_id=0,\n",
    "        scanner=\"ScanImage\",\n",
    "        acq_software=\"ScanImage\",\n",
    "        scan_notes=\"\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we were able to successfully insert data into the scan table. Now we're ready\n",
    "to use DataJoint's automation features to populate several downstream tables after\n",
    "`Scan`.\n",
    "\n",
    "## Populate\n",
    "\n",
    "### Automatically populate tables\n",
    "\n",
    "`scan.ScanInfo` is the first table in the pipeline that can be populated automatically.\n",
    "If a table contains a part table, this part table is also populated during the\n",
    "`populate()` call. Let's view the `scan.ScanInfo` and its part table\n",
    "`scan.ScanInfo.Field`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.ScanInfo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.ScanInfo.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.ScanInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.ScanInfo.Field()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`populate()` takes a few arguments which we will set in the cell below and use it on `ScanInfo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_settings = {\n",
    "                    \"display_progress\": True,\n",
    "                    \"reserve_jobs\": False,\n",
    "                    \"suppress_errors\": False,\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration depends on your network bandwidth to s3\n",
    "scan.ScanInfo.populate(**populate_settings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the information was entered into each of these tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.ScanInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.ScanInfo.Field()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready to perform image processing with `Suite2p`. An important step before\n",
    "processing is managing the parameters which will be used in that step. To do so, we will\n",
    "import suite2p and insert the default parameters into a DataJoint table\n",
    "`ProcessingParamSet`. This table keeps track of all combinations of your image processing\n",
    "parameters. You can choose which parameters are used during processing in a later step.\n",
    "\n",
    "Let's view the attributes and insert data into `imaging.ProcessingParamSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.ProcessingParamSet.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.ProcessingParamSet.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import suite2p\n",
    "\n",
    "params_suite2p = suite2p.default_ops()\n",
    "params_suite2p['nonrigid']=False\n",
    "\n",
    "imaging.ProcessingParamSet.insert_new_params(\n",
    "    processing_method=\"suite2p\",\n",
    "    paramset_idx=0,\n",
    "    params=params_suite2p,\n",
    "    paramset_desc=\"Calcium imaging analysis with Suite2p using default Suite2p parameters\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've inserted suite2p parameters into the `ProcessingParamSet` table,\n",
    "we're almost ready to run image processing. DataJoint uses the `ProcessingTask` table to\n",
    "determine which `ProcessingParamSet` should be used during processing. \n",
    "\n",
    "The `ProcessingTask` table is important for defining several important aspects of\n",
    "downstream processing. Let's view the attributes to get a better understanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.ProcessingTask.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.ProcessingTask.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ProcessingParamSet` table adds two important attributes: \n",
    "+ `paramset_idx` \n",
    "+ `task_mode` \n",
    "\n",
    "The `paramset_idx` attribute is intended to help track\n",
    "your image processing parameter sets. You can also choose the parameter sets on which\n",
    "you want to run the image processing analysis based on this attribute. \n",
    "\n",
    "The `task_mode` attribute can be set to either `load` or `trigger`. When set to `load`,\n",
    "running the processing step initiates a search for exisiting output files of the image\n",
    "processing algorithm defined in `ProcessingParamSet`. When set to `trigger`, the\n",
    "processing step will run image processing on the raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.ProcessingTask.insert1(\n",
    "    dict(\n",
    "        **session_key,\n",
    "        scan_id=0,\n",
    "        paramset_idx=0,\n",
    "        task_mode='load', # load or trigger\n",
    "        processing_output_dir=\"subject1/session1/suite2p\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we are setting `task_mode` to `load`. Let's call populate on the `Processing`\n",
    "table in the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.Processing.populate(**populate_settings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While processing is complete in the step above, you can optionally curate the output of\n",
    "image processing using the `Curation` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.Curation.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.Curation.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.Curation.insert1(\n",
    "    dict(\n",
    "        **session_key,\n",
    "        scan_id=0,\n",
    "        paramset_idx=0,\n",
    "        curation_id=0,\n",
    "        curation_time=\"2021-04-30 12:22:15.032\",\n",
    "        curation_output_dir=\"subject1/session1/suite2p\",\n",
    "        manual_curation=False,\n",
    "        curation_note=\"\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll populate several tables that store the output of image processing, including\n",
    "`MotionCorrection`, `Segementation`, `Fluorescence`, and `Activity`.\n",
    "\n",
    "Feel free to create new cells in this notebook to view each table's dependencies and\n",
    "attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.MotionCorrection.populate(**populate_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.Segmentation.populate(**populate_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.Fluorescence.populate(**populate_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.Activity.populate(**populate_settings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that data is in the DataJoint tables, the analysis workflows such as exploratory\n",
    "analysis, alignment to behavior, statistical testing or modelling, and other downstream\n",
    "analyses can be performed. \n",
    "\n",
    "## Analyze\n",
    "\n",
    "### Query and fetch\n",
    "\n",
    "The next section of this tutorial focuses on working with data that is already in the\n",
    "database. \n",
    "\n",
    "DataJoint queries allow you to view and import data from the database into a python\n",
    "variable using the `fetch()` method. \n",
    "\n",
    "There are several important features supported by `fetch()`:\n",
    "+ By default, an empty `fetch()` imports a list of dictionaries containing all\n",
    "  attributes of all entries in the table that is queried.\n",
    "+ **`fetch1()`**, on the other hand, imports a dictionary containing all attributes of\n",
    "  one of the entries in the table. By default, if a table has multiple entries,\n",
    "  `fetch1()` imports the first entry in the table.\n",
    "+ Both `fetch()` and `fetch1()` accept table attributes as an argument to query values\n",
    "  of that particular attribute.\n",
    "+ Recommended best practice is to **restrict** queries by primary key attributes of the\n",
    "  table to ensure the accuracy of imported data. \n",
    "    + DataJoint offers a convenient way to fetch the primary key attributes of a table\n",
    "      using `fetch(\"KEY\")`.\n",
    "\n",
    "Let's bring together these concepts of querying together by fetching a\n",
    "fluorescence trace with `mask` number 10 into a variable `fluor_trace`, and then\n",
    "plotting the activity trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluor_trace = (imaging.Fluorescence.Trace & \"mask = '10'\").fetch1(\"fluorescence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fluor_trace)\n",
    "plt.title(\"Fluorescence trace for mask 10\");\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"a.u.\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataJoint queries can be far more complex than the example above. After all, plotting\n",
    "traces is likely just the start of your analysis workflow. The examples below perform\n",
    "several operations using DataJoint queries: \n",
    "+ Use multiple restrictions to fetch one image into the variable `average_image`.\n",
    "+ Use a join operation and multiple restrictions to fetch mask coordinates from\n",
    "  `imaging.Segmentation.Mask` and overlay these with the average image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_image = (\n",
    "    imaging.MotionCorrection.Summary & session_key & \"field_idx=0\"\n",
    ").fetch1(\"average_image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(average_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_xpix, mask_ypix = (\n",
    "    imaging.Segmentation.Mask * imaging.MaskClassification.MaskType\n",
    "    & session_key\n",
    "    & \"mask_center_z=0\"\n",
    "    & \"mask_npix > 130\"\n",
    ").fetch(\"mask_xpix\", \"mask_ypix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_image = np.zeros(np.shape(average_image), dtype=bool)\n",
    "for xpix, ypix in zip(mask_xpix, mask_ypix):\n",
    "    mask_image[ypix, xpix] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(average_image)\n",
    "plt.contour(mask_image, colors=\"white\", linewidths=0.5);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more example using complex queries - plot fluorescence and deconvolved activity\n",
    "traces: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curation_key = (imaging.Curation & session_key & \"curation_id=0\").fetch1(\"KEY\")\n",
    "query_cells = (\n",
    "    imaging.Segmentation.Mask * imaging.MaskClassification.MaskType\n",
    "    & curation_key\n",
    "    & \"mask_center_z=0\"\n",
    "    & \"mask_npix > 130\"\n",
    ").proj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluorescence_traces = (imaging.Fluorescence.Trace & query_cells).fetch(\n",
    "    \"fluorescence\", order_by=\"mask\"\n",
    ")\n",
    "\n",
    "activity_traces = (imaging.Activity.Trace & query_cells).fetch(\n",
    "    \"activity_trace\", order_by=\"mask\"\n",
    ")\n",
    "\n",
    "sampling_rate = (scan.ScanInfo & curation_key).fetch1(\"fps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 4))\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "for f, a in zip(fluorescence_traces, activity_traces):\n",
    "    ax.plot(np.r_[: f.size] * 1 / sampling_rate, f, \"k\", label=\"fluorescence trace\")\n",
    "    ax2.plot(\n",
    "        np.r_[: a.size] * 1 / sampling_rate,\n",
    "        a,\n",
    "        \"r\",\n",
    "        alpha=0.5,\n",
    "        label=\"deconvolved trace\",\n",
    "    )\n",
    "\n",
    "    break\n",
    "\n",
    "ax.tick_params(labelsize=14)\n",
    "ax2.tick_params(labelsize=14)\n",
    "\n",
    "ax.legend(loc=\"upper left\", prop={\"size\": 14})\n",
    "ax2.legend(loc=\"upper right\", prop={\"size\": 14})\n",
    "\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"Activity (a.u.)\")\n",
    "ax2.set_ylabel(\"Activity (a.u.)\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning data to events\n",
    "\n",
    "Before analyzing your data, it could be necessary to perform alignment to output of\n",
    "operant behavior devices or other events such as visual or acoustic stimulus.\n",
    "DataJoint's queries simplify this task. \n",
    "\n",
    "Below, we will populate tables with behavior-related activity information in the `Trial`\n",
    "and `Event` schemas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop schemas\n",
    "\n",
    "+ Schemas are not typically dropped in a production workflow with real data in it. \n",
    "+ At the developmental phase, it might be required for the table redesign.\n",
    "+ When dropping all schemas is needed, the following is the dependency order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_databases(databases):\n",
    "    import pymysql.err\n",
    "    conn = dj.conn()\n",
    "\n",
    "    with dj.config(safemode=False):\n",
    "        for database in databases:\n",
    "            schema = dj.Schema(f'{dj.config[\"custom\"][\"database.prefix\"]}{database}')\n",
    "            while schema.list_tables():\n",
    "                for table in schema.list_tables():\n",
    "                    try:\n",
    "                        conn.query(f\"DROP TABLE `{schema.database}`.`{table}`\")\n",
    "                    except pymysql.err.OperationalError:\n",
    "                        print(f\"Can't drop `{schema.database}`.`{table}`. Retrying...\")\n",
    "            schema.drop()\n",
    "\n",
    "# drop_databases(databases=['imaging_report', 'imaging', 'scan', 'session', 'subject', 'lab', 'reference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
